{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db3fa3c-70f2-455c-831f-4b0db97eb22b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given a dataset where in name field, we are seeing unwanted data (Di,.byanshu, S..,hiv, like this..) using SQL or Pyspark, how can we clean this ? this cleaning should be done for 100k of recordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "444ae9b9-042a-4881-9534-d8ad688f23e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *;\n",
    "data = [(\"Di,.byanshu\",), (\"S..,hiv\",), (\"A..nkit\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "cleaned_df = df.withColumn(\n",
    "    \"clean_name\",\n",
    "    trim(regexp_replace(\"name\", \"[^a-zA-Z ]\", \"\"))  # remove everything except alphabets/spaces\n",
    ")\n",
    "\n",
    "cleaned_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404f792d-86a6-48de-9de9-c34f8c02f6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create table\n",
    "CREATE TABLE data_engineering_practice.sql.names_table (\n",
    "    name STRING\n",
    ");\n",
    "\n",
    "-- Insert messy names\n",
    "INSERT INTO data_engineering_practice.sql.names_table (name) VALUES\n",
    "('Di,.byanshu'),\n",
    "('S..,hiv'),\n",
    "('An,,kit'),\n",
    "('Pri.,ya'),\n",
    "('Ro..hit'),\n",
    "('Sh,.,reya'),\n",
    "('Su.,,raj'),\n",
    "('A..nanya');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21243140-31c2-42cd-b36c-5ff3593a418b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757375174838}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    name, TRIM(REGEXP_REPLACE(name, '[^a-zA-Z]', '')) AS clean_name\n",
    "FROM data_engineering_practice.sql.names_table;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06884a06-2b1a-44c5-bd5e-1171ec033ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Find duplicate rows in a table\n",
    "\n",
    "Question: Identify rows that occur more than once in the customers table (based on all columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17ced92-4501-4dd5-8ac4-3ef327074b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"name\", \"email\") \\\n",
    "  .count() \\\n",
    "  .filter(\"count > 1\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ead5556-dfde-4626-a6a6-3482eb351216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT name, email, COUNT(*) AS cnt\n",
    "FROM customers\n",
    "GROUP BY name, email\n",
    "HAVING COUNT(*) > 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1beeb7-eac0-4a19-ae6c-9046bb642f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Delete duplicate rows but keep one (deduplicate)\n",
    "\n",
    "Question: Remove duplicates from transactions, keeping only one row per txn_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcae4fe3-778d-4dd1-a315-4c8fedc8fdb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WITH cte AS (\n",
    "  SELECT *, ROW_NUMBER() OVER(PARTITION BY txn_id ORDER BY txn_time DESC) AS rn\n",
    "  FROM transactions\n",
    ")\n",
    "DELETE FROM transactions WHERE rn > 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1057c523-5192-4b6a-9327-e7fffc4c0c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"txn_id\").orderBy(F.col(\"txn_time\").desc())\n",
    "\n",
    "df = df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "       .filter(\"rn = 1\") \\\n",
    "       .drop(\"rn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd969a94-b18d-4c88-8021-2fd32903a69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Find first occurrence and remove later duplicates\n",
    "\n",
    "Question: From employees, keep only the first record of each employee based on hire_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9fc7399-e446-4e3b-9d65-9995001a6d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM (\n",
    "   SELECT *, ROW_NUMBER() OVER(PARTITION BY emp_id ORDER BY hire_date ASC) AS rn\n",
    "   FROM employees\n",
    ") t\n",
    "WHERE rn = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870e9338-71a5-47a8-bcb6-8a795aaa828e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"emp_id\").orderBy(\"hire_date\")\n",
    "\n",
    "df = df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "       .filter(\"rn = 1\") \\\n",
    "       .drop(\"rn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420067e2-b672-4f29-9150-b6c3c3ccc898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Count how many duplicates each record has\n",
    "\n",
    "Question: For orders, show each order_id with a count of how many duplicates it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "787f70b6-37fa-46c8-99e3-b22f444265da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT order_id, COUNT(*) AS dup_count\n",
    "FROM orders\n",
    "GROUP BY order_id\n",
    "HAVING COUNT(*) > 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81920bbe-45e0-4789-ab8c-303cbad34d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"order_id\") \\\n",
    "  .count() \\\n",
    "  .filter(\"count > 1\") \\\n",
    "  .withColumnRenamed(\"count\", \"dup_count\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d6300e-f0c7-4ce7-a652-d0999717af8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Deduplicate based on multiple columns\n",
    "\n",
    "Question: In sales, remove duplicates where both customer_id and product_id repeat, but keep the latest sale_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee8bcca0-9289-4690-bb21-cada68f336be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM (\n",
    "   SELECT *, ROW_NUMBER() OVER(PARTITION BY customer_id, product_id ORDER BY sale_date DESC) AS rn\n",
    "   FROM sales\n",
    ") t\n",
    "WHERE rn = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27cecab8-19f1-4092-a2d3-4f548d9d0857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"customer_id\", \"product_id\").orderBy(F.col(\"sale_date\").desc())\n",
    "\n",
    "df = df.withColumn(\"rn\", F.row_number().over(w)) \\\n",
    "       .filter(\"rn = 1\") \\\n",
    "       .drop(\"rn\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7263678044077716,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
